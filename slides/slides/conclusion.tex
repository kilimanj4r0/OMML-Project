\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
    \item Accelerated gradient clipping in stochastic optimization significantly enhances performance in the presence of heavy-tailed noise
    \item First high-probability complexity bounds (on a number of oracle calls) were derived by authors for \texttt{clipped-SSTM} and \texttt{clipped-SGD} methods
    \item Experiments from paper are easily reproducible
    \item Our experiments in NLP domain shows noticeable improvement in loss convergence for AdamW optimizer with clipping, while for SGD variants optimizers clipping effect is not significant
\end{itemize}
\end{frame}
